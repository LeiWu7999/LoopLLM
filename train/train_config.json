{
    "data_params": {
        "data_name_or_path": "/root/loop_llm_exp/datasets/packed_data/",
        "max_length": 8192
    },
    "training_params": {
        "output_dir_template": "./my_loop_llama_output_f{}_{}_dynamic",
        "num_train_epochs": 1,
        "per_device_train_batch_size": 1,
        "gradient_accumulation_steps": 32,
        "gradient_checkpointing": true,
        "learning_rate": 2e-5,
        "warmup_steps": 2000,
        "weight_decay": 0.1,
        "adam_beta1": 0.9,
        "adam_beta2": 0.95,
        "adam_epsilon": 1e-5,
        "lr_scheduler_type": "cosine",
        "report_to": "tensorboard",
        "logging_steps": 10,
        "save_strategy": "steps",
        "save_steps": 100,
        "logging_dir_template": "./logs_{}",
        "eval_strategy": "steps",
        "eval_steps": 100,
        "save_total_limit": 5,
        "bf16": true,
        "dataloader_pin_memory": false,
        "remove_unused_columns": false
    },
    "loop_config": {
        "loop_layers": [[6, 8]],
        "loop_strategy": "fixed_count",
        "loop_count": [5],
        "use_dynamic_loop_sampling": true,
        "use_kv_cache_in_training": false
    },
    "dynamic_sampling_params": {
        "r_bar": 5.0,
        "sigma": 0.5,
        "max_loops": 20
    },
    "model_config": {
        "model_name_or_path": "/root/models/Llama-3.2-1B"
    },
    "ppl_eval_config": {
        "enabled": true,
        "eval_loop_counts": [1, 4, 8, 12, 16],
        "text_column": "text",
        "window_size": 2048,
        "stride": 512
    }
} 