#!/usr/bin/env python3
"""
LoopLLMå¯è§£é‡Šæ€§åˆ†æè„šæœ¬
åˆ†ææ¯ä¸€å±‚çš„éšçŠ¶æ€ç»è¿‡lm_headåçš„æ¦‚ç‡åˆ†å¸ƒï¼Œè®¡ç®—KLæ•£åº¦å¹¶å¯è§†åŒ–
"""
import os
# æŒ‚æ¢¯å­åŠ è½½æ•°æ®é›†
# os.environ["http_proxy"] = "http://127.0.0.1:7891"
# os.environ["https_proxy"] = "http://127.0.0.1:7891"
import sys

# æ·»åŠ LoopLLMç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import gc
import signal
import time
import psutil
import json
from datasets import load_dataset
from transformers import AutoTokenizer
from typing import List, Dict, Any
import warnings
from scipy import interpolate
from scipy.optimize import curve_fit
from tqdm import tqdm, trange
from dataclasses import dataclass
import hydra
from omegaconf import DictConfig, OmegaConf
warnings.filterwarnings('ignore')

from loop_llama_config import LoopLlamaConfig
from loop_llama_model import LoopLlamaForCausalLM
from transformers import LlamaConfig


def memory_monitor():
    """ç›‘æ§å†…å­˜ä½¿ç”¨"""
    process = psutil.Process()
    memory_gb = process.memory_info().rss / 1024**3
    # print(f"å½“å‰å†…å­˜ä½¿ç”¨: {memory_gb:.2f} GB")
    if memory_gb > 200:  # å¦‚æœè¶…è¿‡200GBåˆ™è­¦å‘Š
        print("âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå»ºè®®å‡å°‘æ•°æ®é‡")
    return memory_gb


def timeout_handler(signum, frame):
    """è¶…æ—¶å¤„ç†å‡½æ•°"""
    raise TimeoutError("æ“ä½œè¶…æ—¶")


class LoopLLMInterpreter:
    """LoopLLMå¯è§£é‡Šæ€§åˆ†æå™¨"""
    
    def __init__(self, model_config: LoopLlamaConfig, model_path: str, output_dir: str = "interpretability_results"):
        """åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            model_config: LoopLLMæ¨¡å‹é…ç½®
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        """
        self.config = model_config
        self.output_dir = output_dir
        print("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.model = LoopLlamaForCausalLM.from_pretrained(model_path, config=model_config, device_map="auto")
            self.model.eval()
            print("âœ“ æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        # å­˜å‚¨æ¯å±‚çš„éšçŠ¶æ€
        self.layer_hidden_states = []
        self.create_output_directory()
        
    def create_output_directory(self) -> str:
        """åˆ›å»ºè¾“å‡ºç›®å½•
        
        Returns:
            åˆ›å»ºçš„å­ç›®å½•è·¯å¾„
        """
        # ç¡®ä¿åŸºç¡€è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•åç§°
        if self.config.loop_layers is not None:
            # æš‚æ—¶åªæœ‰ä¸€ä¸ªå¾ªç¯å—
            start_layer, end_layer = self.config.loop_layers[0]
            loop_count = self.config.loop_count
            subdir_name = f"output_{loop_count}"
        else:
            subdir_name = f"output_no_loop"
        
        # å®Œæ•´çš„å­ç›®å½•è·¯å¾„
        full_subdir_path = os.path.join(self.output_dir, subdir_name)
        
        # åˆ›å»ºå­ç›®å½•
        os.makedirs(full_subdir_path, exist_ok=True)
        print(f"è¾“å‡ºç›®å½•: {full_subdir_path}")
        
        self.output_dir = full_subdir_path
        
    def load_data(self, num_samples: int = 50) -> List[str]:  # é»˜è®¤å‡å°‘åˆ°50
        """åŠ è½½æ•°æ®é›†
        
        Args:
            num_samples: è¦åŠ è½½çš„æ ·æœ¬æ•°é‡
            
        Returns:
            æ–‡æœ¬æ ·æœ¬åˆ—è¡¨
        """
        print(f"æ­£åœ¨åŠ è½½æ•°æ®é›†çš„å‰{num_samples}æ¡æ•°æ®...")
        
        # é¦–å…ˆå°è¯•è¯»å–æœ¬åœ°ä¿å­˜çš„pile_sample_10.jsonæ–‡ä»¶
        local_file = "pile_sample.json"
        if os.path.exists(local_file):
            try:
                print(f"å‘ç°æœ¬åœ°æ–‡ä»¶ {local_file}ï¼Œå°è¯•è¯»å–...")
                with open(local_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # æå–æ–‡æœ¬å†…å®¹
                if isinstance(data, list):
                    # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼
                    texts = []
                    for item in data:
                        if isinstance(item, dict) and 'text' in item:
                            texts.append(item['text'])
                        elif isinstance(item, str):
                            texts.append(item)
                    
                    # æ ¹æ®éœ€è¦çš„æ ·æœ¬æ•°é‡è¿›è¡Œæˆªå–æˆ–é‡å¤
                    if len(texts) >= num_samples:
                        texts = texts[:num_samples]
                    
                    print(f"âœ“ æˆåŠŸä»æœ¬åœ°æ–‡ä»¶åŠ è½½ {len(texts)} æ¡æ•°æ®")
                    return texts
                    
                elif isinstance(data, dict) and 'texts' in data:
                    # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼ŒåŒ…å«textså­—æ®µ
                    texts = data['texts'][:num_samples]
                    print(f"âœ“ æˆåŠŸä»æœ¬åœ°æ–‡ä»¶åŠ è½½ {len(texts)} æ¡æ•°æ®")
                    return texts
                    
                else:
                    print(f"âš ï¸ æœ¬åœ°æ–‡ä»¶æ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œå›é€€åˆ°åœ¨çº¿æ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨prepare_dataset.pyç”Ÿæˆpile_sample.jsonæ–‡ä»¶")
                    
            except Exception as e:
                print(f"âš ï¸ è¯»å–æœ¬åœ°æ–‡ä»¶å¤±è´¥: {e}")
                print("å›é€€åˆ°åœ¨çº¿æ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨prepare_dataset.pyç”Ÿæˆpile_sample.jsonæ–‡ä»¶")
        else:
            print(f"æœ¬åœ°æ–‡ä»¶ {local_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨åœ¨çº¿æ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨prepare_dataset.pyç”Ÿæˆpile_sample.jsonæ–‡ä»¶")
        
        # å›é€€åˆ°åŸæ¥çš„æ•°æ®é›†åŠ è½½æ–¹æ³•
        # è®¾ç½®è¶…æ—¶
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(60)  # 60ç§’è¶…æ—¶
        
        print("å°è¯•åŠ è½½EleutherAI/the_pile_deduplicatedæ•°æ®é›†...")
        # åŠ è½½æ•°æ®é›†
        try:
            dataset = load_dataset("EleutherAI/the_pile_deduplicated", split="train", streaming=True)
            
            # è·å–å‰num_samplesæ¡æ•°æ®
            texts = []
            for i, example in enumerate(dataset):
                if i >= num_samples:
                    break
                texts.append(example['text'])
                
                if (i + 1) % 10 == 0:  # æ›´é¢‘ç¹çš„è¿›åº¦æŠ¥å‘Š
                    print(f"å·²åŠ è½½ {i + 1} æ¡æ•°æ®...")
            
            signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
            print(f"âœ“ æˆåŠŸåŠ è½½ {len(texts)} æ¡æ•°æ®")
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
        return texts
   
    def forward_with_hidden_states(self, input_ids: torch.Tensor) -> Dict[str, Any]:
        """å‰å‘ä¼ æ’­å¹¶è·å–æ‰€æœ‰å±‚çš„éšçŠ¶æ€
        
        Args:
            input_ids: è¾“å…¥tokenå¼ é‡
            
        Returns:
            åŒ…å«æ‰€æœ‰å±‚éšçŠ¶æ€å’Œè¾“å‡ºçš„å­—å…¸
        """
        # print("æ­£åœ¨è¿›è¡Œå‰å‘ä¼ æ’­...")
        
        try:
            with torch.no_grad():
                # è·å–æ¨¡å‹è¾“å‡ºï¼ŒåŒ…æ‹¬æ‰€æœ‰éšçŠ¶æ€
                outputs = self.model(
                    input_ids=input_ids.to(self.model.device),
                    output_hidden_states=True,
                    use_cache=False
                )
                
                hidden_states = outputs.hidden_states  # åŒ…å«æ‰€æœ‰å±‚çš„éšçŠ¶æ€
                logits = outputs.logits
                
                # print(f"âœ“ è·å¾— {len(hidden_states)} å±‚éšçŠ¶æ€")
                
                return {
                    'hidden_states': hidden_states,
                    'logits': logits,
                    'final_hidden_state': outputs.hidden_states[-1]
                }
        except Exception as e:
            print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
            raise
    
    def compute_layer_probabilities(self, hidden_states: List[torch.Tensor]) -> List[torch.Tensor]:
        """è®¡ç®—æ¯å±‚éšçŠ¶æ€ç»è¿‡lm_headåçš„æ¦‚ç‡åˆ†å¸ƒ
        
        Args:
            hidden_states: æ‰€æœ‰å±‚çš„éšçŠ¶æ€åˆ—è¡¨
            
        Returns:
            æ¯å±‚çš„æ¦‚ç‡åˆ†å¸ƒåˆ—è¡¨
        """
        # print("æ­£åœ¨è®¡ç®—æ¯å±‚çš„æ¦‚ç‡åˆ†å¸ƒ...")
        
        probabilities = []
        
        for i, hidden_state in enumerate(hidden_states):
            try:
                # â­ å…³é”®ä¿®æ”¹ï¼šä¸ºæ¯å±‚hidden stateåº”ç”¨normalization
                # è¿™æ ·ç¡®ä¿ä¸æœ€åä¸€å±‚çš„å¤„ç†æ–¹å¼ä¸€è‡´
                normalized_hidden = self.model.model.norm(hidden_state)
                
                # æ·»åŠ è¯Šæ–­ä¿¡æ¯
                # if i == 0 or i == len(hidden_states) - 1:  # åªåœ¨ç¬¬ä¸€å±‚å’Œæœ€åä¸€å±‚æ‰“å°
                #     print(f"\nç¬¬{i+1}å±‚è¯¦ç»†ç»Ÿè®¡:")
                #     print(f"  åŸå§‹hidden_state: mean={hidden_state.mean().item():.6f}, std={hidden_state.std().item():.6f}")
                #     print(f"  æ ‡å‡†åŒ–å: mean={normalized_hidden.mean().item():.6f}, std={normalized_hidden.std().item():.6f}")
                
                # é€šè¿‡lm_headè·å–logits
                logits = self.model.lm_head(normalized_hidden)
                
                # æ·»åŠ logitsç»Ÿè®¡
                # if i == 0 or i == len(hidden_states) - 1:
                #     print(f"  logits: mean={logits.mean().item():.6f}, std={logits.std().item():.6f}")
                #     print(f"  logitsèŒƒå›´: [{logits.min().item():.6f}, {logits.max().item():.6f}]")
                    
                #     # ğŸ” æ£€æŸ¥logitsçš„åˆ†å¸ƒ
                #     logits_sorted, _ = torch.sort(logits.view(-1), descending=True)
                #     print(f"  logitså‰10å¤§å€¼: {logits_sorted[:10].tolist()}")
                #     print(f"  logitså10å°å€¼: {logits_sorted[-10:].tolist()}")
                
                # åº”ç”¨softmaxè·å–æ¦‚ç‡åˆ†å¸ƒ
                probs = F.softmax(logits, dim=-1).cpu()
                
                # æ·»åŠ æ¦‚ç‡åˆ†å¸ƒç»Ÿè®¡
                # if i == 0 or i == len(hidden_states) - 1:
                #     print(f"  probs: mean={probs.mean().item():.8f}, std={probs.std().item():.8f}")
                #     print(f"  æœ€å¤§æ¦‚ç‡: {probs.max().item():.8f}")
                #     print(f"  æœ€å°æ¦‚ç‡: {probs.min().item():.8e}")
                    
                #     # ğŸ” æ£€æŸ¥æ¦‚ç‡åˆ†å¸ƒ
                #     probs_sorted, indices = torch.sort(probs.view(-1), descending=True)
                #     print(f"  æ¦‚ç‡å‰10å¤§å€¼: {probs_sorted[:10].tolist()}")
                #     print(f"  å¯¹åº”çš„token indices: {indices[:10].tolist()}")
                    
                #     # ğŸ” éªŒè¯softmaxè®¡ç®—
                #     print(f"  æ¦‚ç‡æ±‚å’Œ: {probs.sum(dim=-1).mean().item():.6f} (åº”è¯¥â‰ˆ1.0)")
                    
                #     # ğŸ” æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸é«˜çš„logits
                #     max_logit_pos = torch.argmax(logits.view(-1))
                #     max_logit_value = logits.view(-1)[max_logit_pos]
                #     max_prob_value = probs.view(-1)[max_logit_pos]
                #     print(f"  æœ€å¤§logitä½ç½®: {max_logit_pos.item()}, å€¼: {max_logit_value.item():.6f}")
                #     print(f"  å¯¹åº”æ¦‚ç‡: {max_prob_value.item():.8f}")
                    
                #     print(f"  ç†µ: {-(probs * probs.log()).sum(dim=-1).mean().item():.6f}")
                #     print(f"  ç†è®ºæœ€å¤§ç†µ (log({self.config.vocab_size})): {np.log(self.config.vocab_size):.6f}")
                
                probabilities.append(probs)
                
                # if (i + 1) % 5 == 0:
                #     print(f"å·²å¤„ç† {i + 1} å±‚...")  
                    
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                del logits, normalized_hidden
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                gc.collect()
                
            except Exception as e:
                print(f"âŒ å¤„ç†ç¬¬{i+1}å±‚æ—¶å‡ºé”™: {e}")
                raise
        
        return probabilities
    
    def compute_kl_divergence(self, prob_p: torch.Tensor, prob_q: torch.Tensor) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦ KL(P||Q)
        
        Args:
            prob_p: æ¦‚ç‡åˆ†å¸ƒP (çœŸå®åˆ†å¸ƒæˆ–å‚è€ƒåˆ†å¸ƒ)
            prob_q: æ¦‚ç‡åˆ†å¸ƒQ (è¿‘ä¼¼åˆ†å¸ƒæˆ–ç›®æ ‡åˆ†å¸ƒ)
            
        Returns:
            KLæ•£åº¦å€¼ KL(P||Q) = Î£ P(x) * log(P(x)/Q(x))
            
        Note:
            KLæ•£åº¦è¡¡é‡På’ŒQä¹‹é—´çš„å·®å¼‚ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºå·®å¼‚è¶Šå¤§
            KL(P||Q) != KL(Q||P)ï¼Œä¸å¯¹ç§°
        """
        # é¿å…æ•°å€¼ä¸ç¨³å®š
        eps = 1e-10
        prob_p = (prob_p + eps)
        prob_q = (prob_q + eps)
        
        # PyTorchçš„kl_divå‡½æ•°:
        # F.kl_div(input=log(Q), target=P) = KL(P||Q)
        # æ‰€ä»¥æˆ‘ä»¬ä¼ å…¥ log(prob_q) ä½œä¸ºinputï¼Œprob_p ä½œä¸ºtarget
        # é‡æ–°å½’ä¸€åŒ–
        prob_p = prob_p / prob_p.sum(dim=-1, keepdim=True)
        prob_q = prob_q / prob_q.sum(dim=-1, keepdim=True)
        
        # è®¡ç®—KLæ•£åº¦: KL(P||Q) = Î£ P(x) * log(P(x)/Q(x))
        kl_div = torch.sum(prob_p * torch.log(prob_p / prob_q), dim=-1)
        
        return kl_div
        # return kl_div.item()
    
    def plot_layer_vs_final_kl(self, kl_divergences: List[float], save_path: str = None):
        """ç»˜åˆ¶æ¯å±‚ä¸æœ€åä¸€å±‚KLæ•£åº¦çš„æŸ±çŠ¶å›¾
        
        Args:
            kl_divergences: KLæ•£åº¦åˆ—è¡¨
            save_path: ä¿å­˜æ–‡ä»¶åï¼ˆå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
        """
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ–‡ä»¶åï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆ
        if save_path is None:
            if self.config.loop_layers is not None:
                start_layer, end_layer = self.config.loop_layers
                loop_count = self.config.loop_count
                save_path = f"layer_vs_final_kl_{start_layer}_{end_layer}_{loop_count}.png"
            else:
                save_path = f"layer_vs_final_kl_no_loop.png"
        
        # æ„å»ºå®Œæ•´çš„ä¿å­˜è·¯å¾„
        full_save_path = os.path.join(self.output_dir, save_path)
        
        plt.figure(figsize=(14, 8))
        
        num_layers = len(kl_divergences)
        # ä½¿ç”¨ç›¸å¯¹å±‚æ•°ï¼ˆ0åˆ°1ä¹‹é—´ï¼‰
        relative_layer_indices = np.linspace(0, 1, num_layers)
        
        width = 0.02  # è°ƒæ•´æŸ±å­å®½åº¦ä»¥é€‚åº”ç›¸å¯¹å±‚æ•°
        bars = plt.bar(relative_layer_indices, kl_divergences, width=width, alpha=0.8, 
                      color='skyblue', edgecolor='navy', linewidth=1.5, label='KL divergence')
        
        # æ·»åŠ æ›²çº¿æ‹Ÿåˆ
        try:
            # ä½¿ç”¨å¤šé¡¹å¼æ‹Ÿåˆï¼ˆ3æ¬¡å¤šé¡¹å¼ï¼‰
            # degree = min(3, num_layers - 1)  # ç¡®ä¿å¤šé¡¹å¼æ¬¡æ•°ä¸è¶…è¿‡æ•°æ®ç‚¹æ•°-1
            # coeffs = np.polyfit(relative_layer_indices, kl_divergences, degree)
            # poly_func = np.poly1d(coeffs)
            
            # # ç”Ÿæˆå¹³æ»‘çš„æ›²çº¿ç‚¹
            smooth_x = np.linspace(0, 1, 100)
            # smooth_y = poly_func(smooth_x)
            
            # plt.plot(smooth_x, smooth_y, 'r-', linewidth=2.5, alpha=0.8, label=f'{degree}th-degree polynomial fitting')
            
            # åŒæ—¶å°è¯•æ ·æ¡æ’å€¼æ‹Ÿåˆ
            if num_layers >= 4:  # æ ·æ¡æ’å€¼éœ€è¦è‡³å°‘4ä¸ªç‚¹
                spline = interpolate.UnivariateSpline(relative_layer_indices, kl_divergences, s=0, k=min(3, num_layers-1))
                spline_y = spline(smooth_x)
                plt.plot(smooth_x, spline_y, 'g--', linewidth=2, alpha=0.7, label='spline interpolation fitting')
            
        except Exception as e:
            print(f"âš ï¸ æ›²çº¿æ‹Ÿåˆå¤±è´¥: {e}")
        
        # æ·»åŠ å¾ªç¯å±‚æ ‡è®°çº¿
        if self.config.loop_layers is not None:
            start_layer, end_layer = self.config.loop_layers
            real_end_layer = start_layer + self.config.loop_count * (end_layer - start_layer + 1)
            
            # è®¡ç®—å¾ªç¯å±‚åœ¨ç›¸å¯¹ä½ç½®ä¸­çš„åæ ‡
            start_relative = start_layer / (num_layers) if num_layers > 1 else 0
            end_relative = real_end_layer / (num_layers) if num_layers > 1 else 0
            
            # æ·»åŠ å¾ªç¯å¼€å§‹å’Œç»“æŸçš„å‚ç›´çº¿
            plt.axvline(x=start_relative, color='red', linestyle='--', linewidth=2, alpha=0.8)
            if start_layer != real_end_layer:
                plt.axvline(x=end_relative, color='red', linestyle='--', linewidth=2, alpha=0.8)
            
            # æ·»åŠ å¾ªç¯åŒºåŸŸçš„èƒŒæ™¯é«˜äº®
            # plt.axvspan(start_relative, end_relative, alpha=0.1, color='red', 
            #            label=f'Loop Region (Layers {start_layer}-{end_layer})')
        
        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        title_suffix = ""
        if self.config.loop_layers is not None:
            start_layer, end_layer = self.config.loop_layers
            loop_count = self.config.loop_count
            title_suffix = f" (loop layers: {start_layer}-{end_layer}, loop count: {loop_count})"
        
        plt.xlabel('Relative Layer Position', fontsize=14)
        plt.ylabel('KL Divergence', fontsize=14)
        plt.title(f'KL Divergence Distribution Between Each Layer and Final Layer{title_suffix}', fontsize=16, pad=20)
        
        # è®¾ç½®xè½´åˆ»åº¦å’Œæ ‡ç­¾
        tick_positions = np.linspace(0, 1, min(11, num_layers))  # æœ€å¤šæ˜¾ç¤º11ä¸ªåˆ»åº¦
        tick_labels = [f'{pos:.1f}' for pos in tick_positions]
        plt.xticks(tick_positions, tick_labels, fontsize=12)
        plt.yticks(fontsize=12)
        
        # æ·»åŠ ç½‘æ ¼
        plt.grid(True, alpha=0.3, linestyle='--')
        
        # æ·»åŠ å›¾ä¾‹
        plt.legend(fontsize=12)
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        plt.subplots_adjust(bottom=0.15, top=0.9)
        
        # ä¿å­˜å›¾ç‰‡
        plt.savefig(full_save_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.show()
        print(f"æŸ±çŠ¶å›¾å·²ä¿å­˜è‡³: {full_save_path}")
    
    def plot_all_pairs_heatmap(self, kl_matrix: np.ndarray, save_path: str = None):
        """ç»˜åˆ¶æ‰€æœ‰å±‚å¯¹KLæ•£åº¦çš„çƒ­åŠ›å›¾
        
        Args:
            kl_matrix: KLæ•£åº¦çŸ©é˜µ
            save_path: ä¿å­˜æ–‡ä»¶åï¼ˆå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
        """
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ–‡ä»¶åï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆ
        if save_path is None:
            if self.config.loop_layers is not None:
                start_layer, end_layer = self.config.loop_layers
                loop_count = self.config.loop_count
                save_path = f"all_pairs_kl_heatmap_{start_layer}_{end_layer}_{loop_count}.png"
            else:
                save_path = f"all_pairs_kl_heatmap_no_loop.png"
        
        # æ„å»ºå®Œæ•´çš„ä¿å­˜è·¯å¾„
        full_save_path = os.path.join(self.output_dir, save_path)
        
        plt.figure(figsize=(12, 10))
        
        num_layers = kl_matrix.shape[0]
        
        # åˆ›å»ºç›¸å¯¹å±‚æ•°çš„åˆ»åº¦æ ‡ç­¾
        relative_positions = np.linspace(0, 1, num_layers)
        relative_labels = [f'{pos:.2f}' for pos in relative_positions]
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        mask = kl_matrix == 0  # é®ç›–æ²¡æœ‰æ•°æ®çš„éƒ¨åˆ†
        
        # ä½¿ç”¨æ›´å¥½çš„é¢œè‰²æ˜ å°„
        heatmap = sns.heatmap(kl_matrix, 
                             annot=False, 
                             fmt='.4f', 
                             cmap='YlOrRd',
                             square=True,
                             mask=mask,
                             cbar_kws={'label': 'KL Divergence'},
                             annot_kws={'size': 9},
                             linewidths=0.5,
                             linecolor='white',
                             xticklabels=relative_labels,
                             yticklabels=relative_labels)
        
        # æ·»åŠ å¾ªç¯å±‚æ ‡è®°çº¿
        if self.config.loop_layers is not None:
            start_layer, end_layer = self.config.loop_layers
            real_end_layer = start_layer + self.config.loop_count * (end_layer - start_layer + 1) - 1
            
            # åœ¨çƒ­åŠ›å›¾ä¸­ï¼Œåæ ‡æ˜¯ä»0å¼€å§‹çš„ç´¢å¼•
            # æ·»åŠ å¾ªç¯å±‚çš„è¾¹ç•Œçº¿
            plt.axhline(y=start_layer + 0.5, color='red', linestyle='--', linewidth=2, alpha=0.8)
            plt.axhline(y=real_end_layer + 1.5, color='red', linestyle='--', linewidth=2, alpha=0.8)
            plt.axvline(x=start_layer + 0.5, color='red', linestyle='--', linewidth=2, alpha=0.8)
            plt.axvline(x=real_end_layer + 1.5, color='red', linestyle='--', linewidth=2, alpha=0.8)
            
            # æ·»åŠ å¾ªç¯åŒºåŸŸçš„è¾¹æ¡†é«˜äº®
            # from matplotlib.patches import Rectangle
            # rect = Rectangle((start_layer + 0.5, start_layer + 0.5), 
            #                end_layer - start_layer + 1, 
            #                end_layer - start_layer + 1,
            #                linewidth=3, edgecolor='red', facecolor='none', alpha=0.8)
            # plt.gca().add_patch(rect)
            
            # æ·»åŠ æ–‡æœ¬æ ‡æ³¨
            # plt.text(start_layer + 0.5, -0.5, f'Loop Start\n(Layer {start_layer})', 
            #         ha='center', va='top', color='red', fontweight='bold', fontsize=10)
            # if start_layer != end_layer:
            #     plt.text(end_layer + 0.5, -0.5, f'Loop End\n(Layer {end_layer})', 
            #             ha='center', va='top', color='red', fontweight='bold', fontsize=10)
        
        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        title_suffix = ""
        if self.config.loop_layers is not None:
            start_layer, end_layer = self.config.loop_layers
            loop_count = self.config.loop_count
            title_suffix = f" (loop layers: {start_layer}-{end_layer}, loop count: {loop_count})"
        
        plt.xlabel('Relative Layer Position', fontsize=14)
        plt.ylabel('Relative Layer Position', fontsize=14)
        plt.title(f'KL Divergence Heatmap Between Layer Pairs{title_suffix}', fontsize=16, pad=20)
        
        # è®¾ç½®åˆ»åº¦
        plt.xticks(fontsize=10, rotation=45)
        plt.yticks(fontsize=10)
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        plt.subplots_adjust(bottom=0.15, top=0.9)
        
        # ä¿å­˜å›¾ç‰‡
        plt.savefig(full_save_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.show()
        print(f"çƒ­åŠ›å›¾å·²ä¿å­˜è‡³: {full_save_path}")
    
    @torch.no_grad()
    def run_analysis(self, num_samples: int = 50, batch_size: int = 2):
        """è¿è¡Œå®Œæ•´çš„å¯è§£é‡Šæ€§åˆ†æ
        
        Args:
            num_samples: æ•°æ®æ ·æœ¬æ•°é‡
        """
        print("å¼€å§‹LoopLLMå¯è§£é‡Šæ€§åˆ†æ...")
        print("=" * 50)
        
        # 1. åŠ è½½æ•°æ®
        try:
            texts = self.load_data(num_samples)
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
        
        # 2. å‡†å¤‡è¾“å…¥ (ä½¿ç”¨æ›´å°çš„batchä»¥èŠ‚çœå†…å­˜)
        total_batches = (len(texts) + batch_size - 1) // batch_size
        
        print(f"å°†å¤„ç† {total_batches} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹æ¬¡ {batch_size} ä¸ªæ ·æœ¬")
        
        # ç”¨äºç´¯ç§¯æ‰€æœ‰batchçš„KLæ•£åº¦ç»“æœ
        all_pairs_kl_results = {}  # å­˜å‚¨æ‰€æœ‰å±‚å¯¹çš„KLæ•£åº¦
        num_layers = None
        
        try:
            # ä½¿ç”¨tqdmæ˜¾ç¤ºæ‰¹æ¬¡å¤„ç†è¿›åº¦
            batch_progress = tqdm(range(0, len(texts), batch_size), 
                                desc="Processing batches", 
                                unit="batch",
                                total=total_batches)
            
            for i in batch_progress:
                batch_texts = texts[i:i+batch_size]
                batch_num = i//batch_size + 1
                
                # æ›´æ–°è¿›åº¦æ¡æè¿°
                batch_progress.set_description(f"Processing batch {batch_num}/{total_batches}")
                
                inputs = self.tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True, max_length=512)
                input_ids = inputs['input_ids']
                attention_mask = inputs['attention_mask']
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶…å‡ºè¯æ±‡è¡¨èŒƒå›´çš„token
                # if input_ids.max().item() >= self.config.vocab_size:
                #     print(f"âš ï¸ è­¦å‘Š: å‘ç°è¶…å‡ºè¯æ±‡è¡¨èŒƒå›´çš„token!")
                #     print(f"  æœ€å¤§token id: {input_ids.max().item()}")
                #     print(f"  è¯æ±‡è¡¨å¤§å°: {self.config.vocab_size}")
                
                # 3. å‰å‘ä¼ æ’­
                outputs = self.forward_with_hidden_states(input_ids)    # ä¸€ä¸ªé•¿åº¦ä¸ºå±‚æ•°çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º[batch_size, seq_len, hidden_size]
                
                # å¤„ç†attention_maskï¼Œå»é™¤paddingéƒ¨åˆ†
                non_padding_mask = attention_mask.bool()  # ç¡®ä¿æ˜¯boolç±»å‹
                
                # å¯¹æ¯ä¸€å±‚çš„hidden_statesåˆ†åˆ«å¤„ç†
                filtered_hidden_states = []
                for layer_idx, layer_hidden in enumerate(outputs['hidden_states']):
                    # layer_hidden shape: [batch_size, seq_len, hidden_size]
                    # attention_mask shape: [batch_size, seq_len]
                    
                    # è·å–épaddingä½ç½®çš„hidden states
                    batch_size_cur, seq_len, hidden_size = layer_hidden.shape
                    
                    # å±•å¹³å¤„ç†
                    layer_flat = layer_hidden.view(-1, hidden_size)  # [batch_size * seq_len, hidden_size]
                    mask_flat = non_padding_mask.view(-1)  # [batch_size * seq_len]
                    
                    # é€‰æ‹©épaddingçš„token
                    layer_filtered = layer_flat[mask_flat]  # [num_valid_tokens, hidden_size]
                    filtered_hidden_states.append(layer_filtered)
                
                # print(f'ç¬¬ä¸€å±‚è¿‡æ»¤åçš„hidden_stateså½¢çŠ¶: {filtered_hidden_states[0].shape}')
                
                if num_layers is None:
                    num_layers = len(filtered_hidden_states)
                    # print(f"æ¨¡å‹å…±æœ‰ {num_layers} å±‚")
                
                # 4. ğŸ”¥ åˆå¹¶è®¡ç®—ï¼šè®¡ç®—æ‰€æœ‰å±‚å¯¹çš„KLæ•£åº¦
                batch_pairs_kl = self.compute_all_pairs_kl(filtered_hidden_states)
                
                # 5. ç´¯ç§¯ç»“æœï¼ˆå¯¹æ¯ä¸ªå±‚å¯¹å–å¹³å‡ï¼‰
                for (layer_i, layer_j), kl_value in batch_pairs_kl.items():
                    if (layer_i, layer_j) not in all_pairs_kl_results:
                        all_pairs_kl_results[(layer_i, layer_j)] = []
                    all_pairs_kl_results[(layer_i, layer_j)].append(kl_value)
                
                # å†…å­˜æ¸…ç†
                del outputs, input_ids, attention_mask, filtered_hidden_states
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                gc.collect()
                
                # å¦‚æœå†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œä¸­æ–­å¤„ç†
                # if memory_monitor() > 220:
                #     print("âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œåœæ­¢å¤„ç†æ›´å¤šæ‰¹æ¬¡")
                #     break
                
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
            if not all_pairs_kl_results:
                return None
        
        print("\n" + "=" * 50)
        print("å¼€å§‹æ±‡æ€»ç»“æœ...")
        
        try:
            # 6. æ±‡æ€»æ‰€æœ‰batchçš„KLæ•£åº¦ç»“æœï¼ˆå¯¹æ¯ä¸ªå±‚å¯¹å–å¹³å‡ï¼‰
            final_pairs_kl = {}
            for (layer_i, layer_j), kl_values in all_pairs_kl_results.items():
                final_pairs_kl[(layer_i, layer_j)] = np.mean(kl_values)
            
            print(f"æ±‡æ€»äº† {len(list(all_pairs_kl_results.values())[0])} ä¸ªbatchçš„ç»“æœ")
            
            # 7. ğŸ”¥ ä»å®Œæ•´çš„å±‚å¯¹KLæ•£åº¦ä¸­æå–æ¯å±‚ä¸æœ€åä¸€å±‚çš„KLæ•£åº¦
            layer_vs_final_kl = []
            final_layer_idx = num_layers - 1
            
            for layer_i in range(num_layers):
                if (layer_i, final_layer_idx) in final_pairs_kl:
                    kl_value = final_pairs_kl[(layer_i, final_layer_idx)]
                elif (final_layer_idx, layer_i) in final_pairs_kl:
                    # KLæ•£åº¦ä¸å¯¹ç§°ï¼Œä½†å¦‚æœåªæœ‰åå‘çš„ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨
                    kl_value = final_pairs_kl[(final_layer_idx, layer_i)]
                else:
                    kl_value = 0.0  # å¦‚æœæ²¡æœ‰è®¡ç®—åˆ°ï¼Œè®¾ä¸º0
                
                layer_vs_final_kl.append(kl_value)
            
            # 8. æ„å»ºå®Œæ•´çš„å±‚å¯¹KLæ•£åº¦çŸ©é˜µ
            kl_matrix = np.zeros((num_layers, num_layers))
            for (i, j), kl_value in final_pairs_kl.items():
                kl_matrix[i, j] = kl_value
            
            print(f"âœ“ æå–åˆ°æ¯å±‚ä¸æœ€åä¸€å±‚çš„KLæ•£åº¦: {len(layer_vs_final_kl)} ä¸ªå€¼")
            print(f"âœ“ æ„å»ºå±‚å¯¹KLæ•£åº¦çŸ©é˜µ: {kl_matrix.shape}")
            
        except Exception as e:
            print(f"âŒ ç»“æœæ±‡æ€»å¤±è´¥: {e}")
            return None
        
        print("\n" + "=" * 50)
        print("å¼€å§‹å¯è§†åŒ–...")
        
        try:
            # 9. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
            self.plot_layer_vs_final_kl(layer_vs_final_kl)
            if final_pairs_kl:
                self.plot_all_pairs_heatmap(kl_matrix)
        except Exception as e:
            print(f"âš ï¸ å¯è§†åŒ–å¤±è´¥: {e}")
        
        print("\n" + "=" * 50)
        print("ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶...")
        
        try:
            # 10. ä¿å­˜KLæ•£åº¦ç»“æœåˆ°JSONæ–‡ä»¶
            results_dict = {
                'config': {
                    'loop_layers': self.config.loop_layers,
                    'loop_count': self.config.loop_count,
                    'num_layers': num_layers,
                    'num_samples': num_samples,
                    'batch_size': batch_size
                },
                'layer_vs_final_kl': layer_vs_final_kl,
                'all_pairs_kl_matrix': kl_matrix.tolist(),  # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨
            }
            
            # ç”ŸæˆJSONæ–‡ä»¶å
            if self.config.loop_layers is not None:
                start_layer, end_layer = self.config.loop_layers
                json_filename = f"all_pairs_kl_{start_layer}_{end_layer}_{self.config.loop_count}.json"
            else:
                json_filename = f"all_pairs_kl_no_loop.json"
            
            json_filepath = os.path.join(self.output_dir, json_filename)
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            with open(json_filepath, 'w', encoding='utf-8') as f:
                json.dump(results_dict, f, indent=2, ensure_ascii=False)
                
            print(f"âœ“ KLæ•£åº¦ç»“æœå·²ä¿å­˜åˆ°: {json_filepath}")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
        
        print("\n" + "=" * 50)
        print("åˆ†æå®Œæˆ!")
        
        return {
            'layer_vs_final_kl': layer_vs_final_kl,
            'all_pairs_kl': kl_matrix,
            'pairs_kl_dict': final_pairs_kl  # é¢å¤–è¿”å›å­—å…¸å½¢å¼çš„ç»“æœ
        }
    
    def compute_all_pairs_kl(self, hidden_states: List[torch.Tensor]) -> Dict[tuple, float]:
        """è®¡ç®—æ‰€æœ‰å±‚å¯¹çš„KLæ•£åº¦
        
        Args:
            hidden_states: æ‰€æœ‰å±‚çš„éšçŠ¶æ€åˆ—è¡¨
            
        Returns:
            å±‚å¯¹KLæ•£åº¦å­—å…¸
        """
        # print("æ­£åœ¨è®¡ç®—æ‰€æœ‰å±‚å¯¹çš„KLæ•£åº¦...")
        
        # è®¡ç®—æ‰€æœ‰å±‚çš„æ¦‚ç‡åˆ†å¸ƒ
        probabilities = self.compute_layer_probabilities(hidden_states)
        
        num_layers = len(probabilities)
        kl_results = {}
        
        # éšæœºé‡‡æ ·ä½ç½®ä»¥å‡å°‘è®¡ç®—é‡
        num_tokens = probabilities[0].shape[0]
        # sample_size = min(50, num_tokens)  # é‡‡æ ·æ•°é‡
        sample_size = num_tokens
        indices = torch.randperm(num_tokens)[:sample_size]
        
        # print(f"ä» {num_tokens} ä¸ªtokenä¸­é‡‡æ · {sample_size} ä¸ªè¿›è¡ŒKLæ•£åº¦è®¡ç®—")
        
        for i in range(num_layers):
            for j in range(num_layers):
                if i == j:
                    kl_results[(i, j)] = 0.0
                else:
                    # å¯¹é‡‡æ ·ä½ç½®æ±‚å¹³å‡KLæ•£åº¦
                    batch_kl = []
                    
                    for idx in indices:
                        try:
                            # è®¡ç®— KL(layer_i || layer_j)
                            # probabilities[i][idx] ä½œä¸ºå‚è€ƒåˆ†å¸ƒPï¼Œprobabilities[j][idx] ä½œä¸ºç›®æ ‡åˆ†å¸ƒQ
                            kl = self.compute_kl_divergence(
                                probabilities[i][idx],  # P: å‚è€ƒåˆ†å¸ƒï¼ˆå±‚iï¼‰
                                probabilities[j][idx]   # Q: ç›®æ ‡åˆ†å¸ƒï¼ˆå±‚jï¼‰
                            )
                            batch_kl.append(kl.item() if isinstance(kl, torch.Tensor) else kl)
                        except Exception as e:
                            continue
                    
                    avg_kl = np.mean(batch_kl) if batch_kl else 0.0
                    kl_results[(i, j)] = avg_kl
        
        # print(f"æ‰€æœ‰å±‚å¯¹KLæ•£åº¦è®¡ç®—å®Œæˆï¼Œå…± {len(kl_results)} ä¸ªå±‚å¯¹")
        return kl_results


@hydra.main(version_base=None, config_path="./config", config_name="config")
def main(cfg: DictConfig = None):
    """ä¸»å‡½æ•°"""
    print("LoopLLMå¯è§£é‡Šæ€§åˆ†æå·¥å…·")
    print("=" * 50)
    

    model_path = cfg.model.model_path

    llama_config = LlamaConfig.from_pretrained(model_path)
    config_dict = llama_config.to_dict()

    loop_layers = cfg.loop.loop_layer
    if isinstance(loop_layers, int):
        loop_layers = [loop_layers]
    elif isinstance(loop_layers, str):
        if loop_layers == "all":
            loop_layers = list(range(0, llama_config.num_hidden_layers))
        else:
            loop_layers = None

    if loop_layers is None:
        loop_llama_config = LoopLlamaConfig(
            # LoopLLMç‰¹å®šé…ç½®
            loop_layers=None, 
            loop_strategy="fixed_count",
            loop_count=cfg.loop.loop_count,  # ä»é…ç½®ä¸­è·å–å¾ªç¯æ¬¡æ•°
            kv_cache_mode="virtual_layers",
            virtual_layer_count=cfg.loop.loop_count,  # è™šæ‹Ÿå±‚æ•°é‡ä¸å¾ªç¯æ¬¡æ•°ç›¸åŒ
            min_loop_count=cfg.loop.loop_count,  # æœ€å°å¾ªç¯æ¬¡æ•°ä¸å¾ªç¯æ¬¡æ•°ç›¸åŒ
            max_loop_count=cfg.loop.loop_count,  # æœ€å¤§å¾ªç¯æ¬¡æ•°
            **config_dict
        )           
        
        # åˆ›å»ºåˆ†æå™¨ï¼ŒæŒ‡å®šè¾“å‡ºç›®å½•
        interpreter = LoopLLMInterpreter(loop_llama_config, model_path=model_path, output_dir=cfg.base.output_dir)
        
        # è¿è¡Œåˆ†æ - ä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°
        results = interpreter.run_analysis(num_samples=cfg.base.num_samples, batch_size=cfg.base.batch_size)
    else:
        print(f"éœ€è¦å¾ªç¯çš„å±‚: {loop_layers}, å…±{len(loop_layers)}å±‚")
        print(f"å¾ªç¯æ¬¡æ•°: {cfg.loop.loop_count}")
        
        for loop_layer in loop_layers:
            print(f"æ­£åœ¨åˆ†æç¬¬{loop_layer}å±‚")

            loop_llama_config = LoopLlamaConfig(
                # LoopLLMç‰¹å®šé…ç½®
                loop_layers=[(loop_layer, loop_layer)], 
                loop_strategy="fixed_count",
                loop_count=cfg.loop.loop_count,  # ä»é…ç½®ä¸­è·å–å¾ªç¯æ¬¡æ•°
                kv_cache_mode="virtual_layers",
                virtual_layer_count=cfg.loop.loop_count,  # è™šæ‹Ÿå±‚æ•°é‡ä¸å¾ªç¯æ¬¡æ•°ç›¸åŒ
                min_loop_count=cfg.loop.loop_count,  # æœ€å°å¾ªç¯æ¬¡æ•°ä¸å¾ªç¯æ¬¡æ•°ç›¸åŒ
                max_loop_count=cfg.loop.loop_count,  # æœ€å¤§å¾ªç¯æ¬¡æ•°
                **config_dict
            )
            
            # print("æ¨¡å‹é…ç½®:")
            # print(f"  è¯æ±‡è¡¨å¤§å°: {loop_llama_config.vocab_size}")
            # print(f"  éšè—å±‚å¤§å°: {loop_llama_config.hidden_size}")
            # print(f"  å±‚æ•°: {loop_llama_config.num_hidden_layers}")
            # print(f"  å¾ªç¯å±‚: {loop_llama_config.loop_layers}")
            # print(f"  å¾ªç¯æ¬¡æ•°: {loop_llama_config.loop_count}")
                
            
            # åˆ›å»ºåˆ†æå™¨ï¼ŒæŒ‡å®šè¾“å‡ºç›®å½•
            interpreter = LoopLLMInterpreter(loop_llama_config, model_path=model_path, output_dir=cfg.base.output_dir)
            
            # è¿è¡Œåˆ†æ - ä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°
            results = interpreter.run_analysis(num_samples=cfg.base.num_samples, batch_size=cfg.base.batch_size)
            
            # if results:
            #     print("\nåˆ†æç»“æœæ‘˜è¦:")
            #     print(f"æ¯å±‚ä¸æœ€åä¸€å±‚çš„KLæ•£åº¦: {results['layer_vs_final_kl']}")
            #     print(f"KLæ•£åº¦çŸ©é˜µå½¢çŠ¶: {results['all_pairs_kl'].shape}")
            # else:
            #     print("âŒ åˆ†æå¤±è´¥")


if __name__ == "__main__":
    main() 