# LoopLlama: å¾ªç¯å±‚å¢å¼ºçš„LLaMAæ¨¡å‹

## ğŸ“– é¡¹ç›®ç®€ä»‹

LoopLlamaæ˜¯ä¸€ä¸ªåŸºäºLLaMAæ¶æ„çš„åˆ›æ–°è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥**å¾ªç¯å±‚æœºåˆ¶**æ¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œæ¨ç†æ·±åº¦ã€‚è¯¥é¡¹ç›®å®ç°äº†å¤šç§å¾ªç¯ç­–ç•¥å’ŒKVç¼“å­˜ç®¡ç†æ–¹æ¡ˆï¼Œä¸ºæ·±åº¦å­¦ä¹ ç ”ç©¶æä¾›äº†æ–°çš„æ¢ç´¢æ–¹å‘ã€‚

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ„

### æ ¸å¿ƒæ–‡ä»¶ç»“æ„
```
loop_model/
â”œâ”€â”€ loop_llama_config.py    # é…ç½®ç±»ï¼Œå®šä¹‰æ‰€æœ‰å¾ªç¯ç›¸å…³å‚æ•°
â”œâ”€â”€ loop_llama_model.py     # ä¸»æ¨¡å‹å®ç°ï¼ŒåŒ…å«å¾ªç¯å±‚é€»è¾‘
â”œâ”€â”€ loop_cache_utils.py     # KVç¼“å­˜ç®¡ç†ï¼Œæ”¯æŒå¤šç§ç¼“å­˜ç­–ç•¥
â”œâ”€â”€ requirements.txt        # ä¾èµ–åŒ…åˆ—è¡¨
â””â”€â”€ README.md              # æœ¬ä½¿ç”¨æ‰‹å†Œ
```

### å¾ªç¯å±‚æœºåˆ¶
- **å¾ªç¯å±‚èŒƒå›´**: æŒ‡å®šæ¨¡å‹ä¸­å“ªäº›å±‚å‚ä¸å¾ªç¯è®¡ç®—
- **å¾ªç¯ç­–ç•¥**: å›ºå®šæ¬¡æ•°å¾ªç¯ vs åŠ¨æ€æ”¶æ•›åœæ­¢
- **KVç¼“å­˜æ¨¡å¼**: è™šæ‹Ÿå±‚æ˜ å°„ vs åˆå¹¶ç­–ç•¥

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå®‰è£…
```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source loop_env/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. åŸºç¡€ä½¿ç”¨ç¤ºä¾‹
```python
import torch
from loop_llama_config import LoopLlamaConfig
from loop_llama_model import LoopLlamaForCausalLM

# åˆ›å»ºé…ç½®
config = LoopLlamaConfig(
    vocab_size=32000,
    hidden_size=512,
    num_hidden_layers=12,
    num_attention_heads=8,
    loop_layers=(4, 7),  # ç¬¬4-7å±‚ä¸ºå¾ªç¯å±‚
    loop_strategy="fixed_count",
    loop_count=3,
    kv_cache_mode="virtual_layers"
)

# åˆå§‹åŒ–æ¨¡å‹
model = LoopLlamaForCausalLM(config)

# æ¨ç†
input_ids = torch.tensor([[1, 2, 3, 4, 5]])
outputs = model(input_ids=input_ids)
logits = outputs.logits
```

## âš™ï¸ é…ç½®å‚æ•°è¯¦è§£

### åŸºç¡€æ¨¡å‹å‚æ•°
```python
config = LoopLlamaConfig(
    vocab_size=32000,           # è¯æ±‡è¡¨å¤§å°
    hidden_size=512,            # éšè—å±‚ç»´åº¦
    intermediate_size=1024,     # FFNä¸­é—´å±‚ç»´åº¦
    num_hidden_layers=12,       # æ€»å±‚æ•°
    num_attention_heads=8,      # æ³¨æ„åŠ›å¤´æ•°
    max_position_embeddings=2048, # æœ€å¤§ä½ç½®ç¼–ç é•¿åº¦
)
```

### å¾ªç¯å±‚é…ç½®
```python
# å¾ªç¯å±‚èŒƒå›´
loop_layers=(4, 7),  # ç¬¬4-7å±‚ä¸ºå¾ªç¯å±‚ï¼ŒNoneè¡¨ç¤ºæ— å¾ªç¯å±‚

# å¾ªç¯ç­–ç•¥
loop_strategy="fixed_count",     # "fixed_count" | "dynamic_stop"
loop_count=3,                    # å›ºå®šå¾ªç¯æ¬¡æ•°
max_loop_count=10,               # æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰

# åŠ¨æ€åœæ­¢å‚æ•°
cosine_threshold=0.95,           # ä½™å¼¦ç›¸ä¼¼åº¦æ”¶æ•›é˜ˆå€¼
kl_threshold=0.01,               # KLæ•£åº¦æ”¶æ•›é˜ˆå€¼
```

### KVç¼“å­˜æ¨¡å¼
```python
# æ¨¡å¼é€‰æ‹©
kv_cache_mode="virtual_layers",  # "virtual_layers" | "merge_strategy"

# è™šæ‹Ÿå±‚æ¨¡å¼å‚æ•°
virtual_layer_count=5,           # æ¯ä¸ªç‰©ç†å±‚å¯¹åº”çš„è™šæ‹Ÿå±‚æ•°
min_loop_count=5,                # æœ€å°å¾ªç¯æ¬¡æ•°
virtual_attention_mode="parallel", # "parallel" | "serial"

# åˆå¹¶ç­–ç•¥æ¨¡å¼å‚æ•°
merge_strategy="ema",            # "ema" | "average" | "last"
merge_ema_alpha=0.7,             # EMAè¡°å‡ç³»æ•°
```

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### 1. æ–‡æœ¬ç”Ÿæˆ
```python
# é…ç½®ç”Ÿæˆæ¨¡å‹
config = LoopLlamaConfig(
    vocab_size=32000,
    hidden_size=512,
    num_hidden_layers=12,
    num_attention_heads=8,
    loop_layers=(4, 7),
    loop_strategy="dynamic_stop",
    cosine_threshold=0.95,
    kv_cache_mode="virtual_layers",
    virtual_attention_mode="parallel"
)

model = LoopLlamaForCausalLM(config)

# ç”Ÿæˆæ–‡æœ¬
input_ids = torch.tensor([[1, 2, 3]])  # èµ·å§‹token
generated = model.generate(
    input_ids=input_ids,
    max_length=50,
    do_sample=True,
    temperature=0.8
)
```

### 2. å›°æƒ‘åº¦è¯„ä¼°
```python
def calculate_perplexity(model, input_ids):
    model.eval()
    with torch.no_grad():
        outputs = model(input_ids=input_ids, labels=input_ids)
        loss = outputs.loss
        ppl = torch.exp(loss)
    return ppl.item()

# è¯„ä¼°æ¨¡å‹
test_sequences = [
    torch.tensor([[1, 2, 3, 4, 5]]),
    torch.tensor([[10, 20, 30, 40]])
]

for seq in test_sequences:
    ppl = calculate_perplexity(model, seq)
    print(f"åºåˆ— {seq.squeeze().tolist()} çš„PPL: {ppl:.4f}")
```

### 3. æ‰¹é‡å¤„ç†
```python
# æ‰¹é‡æ¨ç†ï¼ˆæ¨èç”¨äºè¯„ä¼°ï¼‰
batch_input_ids = torch.tensor([
    [1, 2, 3, 4, 5, 0, 0],  # paddingåˆ°ç»Ÿä¸€é•¿åº¦
    [10, 20, 30, 40, 50, 60, 70]
])
attention_mask = torch.tensor([
    [1, 1, 1, 1, 1, 0, 0],  # æ ‡è®°æœ‰æ•ˆtoken
    [1, 1, 1, 1, 1, 1, 1]
])

outputs = model(
    input_ids=batch_input_ids,
    attention_mask=attention_mask,
    labels=batch_input_ids
)
batch_loss = outputs.loss
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. è‡ªå®šä¹‰å¾ªç¯ç­–ç•¥
```python
# åŠ¨æ€åœæ­¢ç­–ç•¥
config = LoopLlamaConfig(
    loop_strategy="dynamic_stop",
    cosine_threshold=0.98,      # æ›´ä¸¥æ ¼çš„æ”¶æ•›æ¡ä»¶
    max_loop_count=15,          # å…è®¸æ›´å¤šå¾ªç¯
    kv_cache_mode="merge_strategy",
    merge_strategy="ema",
    merge_ema_alpha=0.8
)
```

### 2. ä¸åŒæ³¨æ„åŠ›æ¨¡å¼å¯¹æ¯”
```python
# å¹¶è¡Œæ³¨æ„åŠ›ï¼šæ‰€æœ‰è™šæ‹Ÿå±‚çš„KVçŠ¶æ€æ‹¼æ¥
config_parallel = LoopLlamaConfig(
    kv_cache_mode="virtual_layers",
    virtual_attention_mode="parallel",
    virtual_layer_count=3
)

# ä¸²è¡Œæ³¨æ„åŠ›ï¼šåªä½¿ç”¨å½“å‰è™šæ‹Ÿå±‚çš„KVçŠ¶æ€
config_serial = LoopLlamaConfig(
    kv_cache_mode="virtual_layers",
    virtual_attention_mode="serial",
    virtual_layer_count=3
)
```

### 3. å†…å­˜ä¼˜åŒ–
```python
# å¯¹äºå¤§æ¨¡å‹ï¼Œä½¿ç”¨åˆå¹¶ç­–ç•¥å‡å°‘å†…å­˜å ç”¨
config = LoopLlamaConfig(
    kv_cache_mode="merge_strategy",
    merge_strategy="last",      # åªä¿ç•™æœ€åä¸€æ¬¡å¾ªç¯ç»“æœ
    loop_count=5
)
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

### 1. å¾ªç¯æ¬¡æ•°ç»Ÿè®¡
```python
# å¯¹äºåŠ¨æ€åœæ­¢ç­–ç•¥ï¼Œå¯ä»¥ç›‘æ§å®é™…å¾ªç¯æ¬¡æ•°
def monitor_loop_steps(model, input_ids):
    model.eval()
    with torch.no_grad():
        outputs = model(input_ids=input_ids, use_cache=True)
        if hasattr(outputs.past_key_values, 'current_forward_loop_step'):
            loop_steps = outputs.past_key_values.current_forward_loop_step
            print(f"å®é™…å¾ªç¯æ¬¡æ•°: {loop_steps}")
    return outputs
```

### 2. å†…å­˜ä½¿ç”¨ç›‘æ§
```python
import torch

def monitor_memory():
    if torch.cuda.is_available():
        print(f"GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        print(f"GPUå†…å­˜ç¼“å­˜: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. é…ç½®å…¼å®¹æ€§
- `virtual_layer_count` å¿…é¡» â‰¤ `min_loop_count`
- `max_loop_count` å¿…é¡» â‰¥ `min_loop_count`
- å¾ªç¯å±‚èŒƒå›´ä¸èƒ½è¶…å‡ºæ¨¡å‹æ€»å±‚æ•°

### 2. å†…å­˜ç®¡ç†
- è™šæ‹Ÿå±‚å¹¶è¡Œæ¨¡å¼å†…å­˜å ç”¨è¾ƒå¤§ï¼Œé€‚åˆå°æ¨¡å‹æˆ–å……è¶³å†…å­˜ç¯å¢ƒ
- åˆå¹¶ç­–ç•¥æ¨¡å¼å†…å­˜å‹å¥½ï¼Œé€‚åˆå¤§æ¨¡å‹éƒ¨ç½²
- é•¿åºåˆ—æ¨ç†æ—¶å»ºè®®ä½¿ç”¨è¾ƒå°çš„`virtual_layer_count`

### 3. æ€§èƒ½æƒè¡¡
- å¾ªç¯æ¬¡æ•°è¶Šå¤šï¼Œè®¡ç®—å¼€é”€è¶Šå¤§ï¼Œä½†å¯èƒ½è·å¾—æ›´å¥½çš„è¡¨ç¤ºèƒ½åŠ›
- åŠ¨æ€åœæ­¢ç­–ç•¥å¯ä»¥è‡ªé€‚åº”è°ƒæ•´è®¡ç®—é‡ï¼Œä½†å¢åŠ äº†åˆ¤æ–­å¼€é”€
- ä¸åŒKVç¼“å­˜æ¨¡å¼é€‚ç”¨äºä¸åŒçš„åº”ç”¨åœºæ™¯

## ğŸ”¬ å®éªŒå»ºè®®

### 1. æ¨¡å‹å¯¹æ¯”å®éªŒ
```python
# åˆ›å»ºå¯¹ç…§ç»„ï¼šæ— å¾ªç¯å±‚çš„åŸºå‡†æ¨¡å‹
baseline_config = LoopLlamaConfig(loop_layers=None)
baseline_model = LoopLlamaForCausalLM(baseline_config)

# å®éªŒç»„ï¼šä¸åŒå¾ªç¯é…ç½®
loop_configs = [
    {"loop_count": 3, "kv_cache_mode": "virtual_layers"},
    {"loop_count": 5, "kv_cache_mode": "merge_strategy"},
    {"loop_strategy": "dynamic_stop", "cosine_threshold": 0.95}
]
```

### 2. è¯„ä¼°æŒ‡æ ‡
- **å›°æƒ‘åº¦(PPL)**: è¯­è¨€å»ºæ¨¡èƒ½åŠ›
- **ç”Ÿæˆè´¨é‡**: BLEUã€ROUGEç­‰æŒ‡æ ‡
- **è®¡ç®—æ•ˆç‡**: æ¨ç†æ—¶é—´ã€å†…å­˜å ç”¨
- **æ”¶æ•›æ€§**: åŠ¨æ€åœæ­¢ç­–ç•¥çš„å¾ªç¯æ¬¡æ•°åˆ†å¸ƒ

## ğŸ“š æ‰©å±•å¼€å‘

### 1. è‡ªå®šä¹‰æ”¶æ•›æ¡ä»¶
å¯ä»¥åœ¨`loop_llama_model.py`çš„`_check_convergence`æ–¹æ³•ä¸­æ·»åŠ æ–°çš„æ”¶æ•›åˆ¤æ–­é€»è¾‘ã€‚

### 2. æ–°çš„åˆå¹¶ç­–ç•¥
å¯ä»¥åœ¨`loop_cache_utils.py`çš„`_merge_current_forward_history`æ–¹æ³•ä¸­å®ç°æ–°çš„KVçŠ¶æ€åˆå¹¶ç®—æ³•ã€‚

### 3. å¾ªç¯å±‚é€‰æ‹©ç­–ç•¥
å¯ä»¥å®ç°åŠ¨æ€é€‰æ‹©å“ªäº›å±‚å‚ä¸å¾ªç¯çš„æœºåˆ¶ï¼Œè€Œä¸æ˜¯å›ºå®šçš„å±‚èŒƒå›´ã€‚

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. ä¿æŒä»£ç é£æ ¼ä¸€è‡´
2. æ·»åŠ å……åˆ†çš„æ³¨é‡Šå’Œæ–‡æ¡£
3. ç¡®ä¿æ–°åŠŸèƒ½æœ‰ç›¸åº”çš„é…ç½®å‚æ•°
4. éªŒè¯åŠŸèƒ½çš„æ­£ç¡®æ€§å’Œæ€§èƒ½å½±å“

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªMITè®¸å¯è¯ï¼Œè¯¦è§LICENSEæ–‡ä»¶ã€‚

---

**LoopLlama** - æ¢ç´¢å¾ªç¯æœºåˆ¶åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ— é™å¯èƒ½ ğŸš€ 